Netty源码，接口，结构。能看懂的话、
--------
1<<n:表示将1的二进制左移n位，1是0001，当n为2时，结果是0100，也就是8.
序列化就是将对象的状态信息转换成可以存储或传输的状态的过程。
--------
IO编程：
使用传统的IO编程，可以实现一个客户端与服务端通信的程序，但是在传统的IO模型中，每个连接创建成功之后都需要一个线程来维护，每个线程包含一个while死循环，那么1w个连接对应1w个线程，继而1w个while死循环，这就带来如下几个问题：
1.线程资源受限：线程是操作系统中非常宝贵的资源，同一时刻有大量的线程处于阻塞状态是非常严重的资源浪费，操作系统耗不起。
2.线程切换效率低下：单机cpu核数固定，线程爆炸之后操作系统频繁进行线程切换，应用性能急剧下降。
3.除了以上两个问题，IO编程中，我们看到数据读写是以字节流为单位，效率不高。
为了解决这三个问题，JDK在1.4之后提出了NIO。
NIO编程：
IO模型中，一个连接来了，会创建一个线程，对应一个while死循环，死循环的目的就是不断监测这条连接上是否有数据可以读，大多数情况下，1w个连接里面同一时刻只有少量的连接有数据可读，因此，很多个while死循环都白白浪费掉了，因为读不出啥数据。
而在NIO模型中，他把这么多while死循环变成一个死循环，这个死循环由一个线程控制，那么他又是如何做到一个线程，一个while死循环就能监测1w个连接是否有数据可读的呢？
这就是NIO模型中selector的作用，一条连接来了之后，现在不创建一个while死循环去监听是否有数据可读了，而是直接把这条连接注册到selector上，然后，通过检查这个selector，就可以批量监测出有数据可读的连接，进而读取数据，下面我再举个非常简单的生活中的例子说明IO与NIO的区别。

在一家幼儿园里，小朋友有上厕所的需求，小朋友都太小以至于你要问他要不要上厕所，他才会告诉你。幼儿园一共有100个小朋友，有两种方案可以解决小朋友上厕所的问题：
每个小朋友配一个老师。每个老师隔段时间询问小朋友是否要上厕所，如果要上，就领他去厕所，100个小朋友就需要100个老师来询问，并且每个小朋友上厕所的时候都需要一个老师领着他去上，这就是IO模型，一个连接对应一个线程。
所有的小朋友都配同一个老师。这个老师隔段时间询问所有的小朋友是否有人要上厕所，然后每一时刻把所有要上厕所的小朋友批量领到厕所，这就是NIO模型，所有小朋友都注册到同一个老师，对应的就是所有的连接都注册到一个线程，然后批量轮询。
这就是NIO模型解决线程资源受限的方案，实际开发过程中，我们会开多个线程，每个线程都管理着一批连接，相对于IO模型中一个线程管理一条连接，消耗的线程资源大幅减少.
由于NIO模型中线程数量大大降低，线程切换效率因此也大幅度提高。
NIO数据读写不再以字节为单位，而是以字节块为单位。IO模型中，每次都是从操作系统底层一个字节一个字节地读取数据，而NIO维护一个缓冲区，每次可以从这个缓冲区里面读取一块的数据，
这就好比一盘美味的豆子放在你面前，你用筷子一个个夹（每次一个），肯定不如要勺子挖着吃（每次一批）效率来得高。
NIO编程的缺点：
1、JDK的NIO编程需要了解很多的概念，编程复杂，对NIO入门非常不友好，编程模型不友好，ByteBuffer的api简直反人类
2、对NIO编程来说，一个比较合适的线程模型能充分发挥它的优势，而JDK没有给你实现，你需要自己实现，就连简单的自定义协议拆包都要你自己实现
3、JDK的NIO底层由epoll实现，该实现饱受诟病的空轮训bug会导致cpu飙升100%
4、项目庞大之后，自行实现的NIO很容易出现各类bug，维护成本较高，上面这一坨代码我都不能保证没有bug
JDK的NIO犹如带刺的玫瑰，虽然美好，让人向往，但是使用不当会让你抓耳挠腮，痛不欲生，正因为如此，Netty横空出世！
Netty编程：
用一句简单的话来说就是：Netty封装了JDK的NIO，让你用得更爽，你不用再写一大堆复杂的代码了。
用官方正式的话来说就是：Netty是一个异步事件驱动的网络应用框架，用于快速开发可维护的高性能服务器和客户端。
服务器代码：
public class NettyServer {
    public static void main(String[] args) {
       
        NioEventLoopGroup boos = new NioEventLoopGroup();
        NioEventLoopGroup worker = new NioEventLoopGroup();
        ServerBootstrap serverBootstrap = new ServerBootstrap();

        serverBootstrap
                .group(boos, worker)
                .channel(NioServerSocketChannel.class)
                .childHandler(new ChannelInitializer<NioSocketChannel>() {
                    protected void initChannel(NioSocketChannel ch) {
                        ch.pipeline().addLast(new StringDecoder());
                        ch.pipeline().addLast(new SimpleChannelInboundHandler<String>() {
                            @Override
                            protected void channelRead0(ChannelHandlerContext ctx, String msg) {
                                System.out.println(msg);
                            }
                        });
                    }
                })
                .bind(8000);
    }
}
这么一小段代码就实现了我们前面NIO编程中的所有的功能，包括服务端启动，接受新连接，打印客户端传来的数据
初学Netty的时候，由于缺乏NIO的编程经验，因此，将Netty里面的概念与IO模型结合起来更好理解
1.boos，对应IOServer.java中的接受新连接线程，主要负责创建新连接。就像老板从外面接活。
2.worker，对应IOClient.java中的负责读取数据的线程，主要用于读取数据以及业务逻辑处理。就像工人工作。


--------
new String(tmp,1,nlen,"UTF8")就是将字节数组tmp从索引的第1位取nlen长度后组成字符串，切组成后的字符串按照utf8的字符集编码，够详细了吧
--------
什么是长连接

     HTTP1.1规定了默认保持长连接（HTTP persistent connection ，也有翻译为持久连接），数据传输完成了保持TCP连接不断开（不发RST包、不四次握手），等待在同域名下继续用这个通道传输数据；相反的就是短连接。

　HTTP首部的Connection: Keep-alive是HTTP1.0浏览器和服务器的实验性扩展，当前的HTTP1.1 RFC2616文档没有对它做说明，因为它所需要的功能已经默认开启，无须带着它，但是实践中可以发现，浏览器的报文请求都会带上它。如果HTTP1.1版本的HTTP请求报文不希望使用长连接，则要在HTTP请求报文首部加上Connection: close。《HTTP权威指南》提到，有部分古老的HTTP1.0 代理不理解Keep-alive，而导致长连接失效：客户端-->代理-->服务端，客户端带有Keep-alive，而代理不认识，于是将报文原封不动转给了服务端，服务端响应了Keep-alive，也被代理转发给了客户端，于是保持了“客户端-->代理”连接和“代理-->服务端”连接不关闭，但是，当客户端第发送第二次请求时，代理会认为当前连接不会有请求了，于是忽略了它，长连接失效。书上也介绍了解决方案：当发现HTTP版本为1.0时，就忽略Keep-alive，客户端就知道当前不该使用长连接。其实，在实际使用中不需要考虑这么多，很多时候代理是我们自己控制的，如Nginx代理，代理服务器有长连接处理逻辑，服务端无需做patch处理，常见的是客户端跟Nginx代理服务器使用HTTP1.1协议&长连接，而Nginx代理服务器跟后端服务器使用HTTP1.0协议&短连接。

    在实际使用中，HTTP头部有了Keep-Alive这个值并不代表一定会使用长连接，客户端和服务器端都可以无视这个值，也就是不按标准来，譬如我自己写的HTTP客户端多线程去下载文件，就可以不遵循这个标准，并发的或者连续的多次GET请求，都分开在多个TCP通道中，每一条TCP通道，只有一次GET，GET完之后，立即有TCP关闭的四次握手，这样写代码更简单，这时候虽然HTTP头有Connection: Keep-alive，但不能说是长连接。正常情况下客户端浏览器、web服务端都有实现这个标准，因为它们的文件又小又多，保持长连接减少重新开TCP连接的开销很有价值。

     以前使用libcurl做的上传/下载，就是短连接，抓包可以看到：1、每一条TCP通道只有一个POST；2、在数据传输完毕可以看到四次握手包。只要不调用curl_easy_cleanup，curl的handle就可能一直有效，可复用。这里说可能，因为连接是双方的，如果服务器那边关掉了，那么我客户端这边保留着也不能实现长连接。    
    如果是使用windows的WinHTTP库，则在POST/GET数据的时候，虽然我关闭了句柄，但这时候TCP连接并不会立即关闭，而是等一小会儿，这时候是WinHTTP库底层支持了跟Keep-alive所需要的功能：即便没有Keep-alive，WinHTTP库也可能会加上这种TCP通道复用的功能，而其它的网络库像libcurl则不会这么做。以前观察过WinHTTP库不会及时断开TCP连接。
--------
三、长连接的数据传输完成识别

    使用长连接之后，客户端、服务端怎么知道本次传输结束呢？两部分：1是判断传输数据是否达到了Content-Length指示的大小；2动态生成的文件没有Content-Length，它是分块传输（chunked），这时候就要根据chunked编码来判断，chunked编码的数据在最后有一个空chunked块，表明本次传输数据结束。更细节的介绍可以看这篇文章。
四、并发连接数的数量限制

    在web开发中需要关注浏览器并发连接的数量，RFC文档说，客户端与服务器最多就连上两通道，但服务器、个人客户端要不要这么做就随人意了，有些服务器就限制同时只能有1个TCP连接，导致客户端的多线程下载（客户端跟服务器连上多条TCP通道同时拉取数据）发挥不了威力，有些服务器则没有限制。浏览器客户端就比较规矩，知乎这里有分析，限制了同域名下能启动若干个并发的TCP连接去下载资源。并发数量的限制也跟长连接有关联，打开一个网页，很多个资源的下载可能就只被放到了少数的几条TCP连接里，这就是TCP通道复用（长连接）。如果并发连接数少，意味着网页上所有资源下载完需要更长的时间（用户感觉页面打开卡了）；并发数多了，服务器可能会产生更高的资源消耗峰值。浏览器只对同域名下的并发连接做了限制，也就意味着，web开发者可以把资源放到不同域名下，同时也把这些资源放到不同的机器上，这样就完美解决了。
五、容易混淆的概念——TCP的keep alive和HTTP的Keep-alive

    TCP的keep alive是检查当前TCP连接是否活着；HTTP的Keep-alive是要让一个TCP连接活久点。它们是不同层次的概念。
    TCP keep alive的表现：
    当一个连接“一段时间”没有数据通讯时，一方会发出一个心跳包（Keep Alive包），如果对方有回包则表明当前连接有效，继续监控。
这个“一段时间”可以设置。
WinHttp库的设置：
WINHTTP_OPTION_WEB_SOCKET_KEEPALIVE_INTERVAL
Sets the interval, in milliseconds, to send a keep-alive packet over the connection. The default interval is 30000 (30 seconds). The minimum interval is 15000 (15 seconds). Using WinHttpSetOption to set a value lower than 15000 will return with ERROR_INVALID_PARAMETER.

libcurl的设置：
http://curl.haxx.se/libcurl/c/curl_easy_setopt.html
CURLOPT_TCP_KEEPALIVE
Pass a long. If set to 1, TCP keepalive probes will be sent. The delay and frequency of these probes can be controlled by the CURLOPT_TCP_KEEPIDLE and CURLOPT_TCP_KEEPINTVL options, provided the operating system supports them. Set to 0 (default behavior) to disable keepalive probes (Added in 7.25.0).
CURLOPT_TCP_KEEPIDLE
Pass a long. Sets the delay, in seconds, that the operating system will wait while the connection is idle before sending keepalive probes. Not all operating systems support this option. (Added in 7.25.0)
CURLOPT_TCP_KEEPINTVL
Pass a long. Sets the interval, in seconds, that the operating system will wait between sending keepalive probes. Not all operating systems support this option. (Added in 7.25.0)
     CURLOPT_TCP_KEEPIDLE是空闲多久发送一个心跳包，CURLOPT_TCP_KEEPINTVL是心跳包间隔多久发一个。 
打开网页抓包，发送心跳包和关闭连接如下：
 

    从上图可以看到，大概过了44秒，客户端发出了心跳包，服务器及时回应，本TCP连接继续保持。到了空闲60秒的时候，服务器主动发起FIN包，断开连接。
六、HTTP 流水线技术

    使用了HTTP长连接（HTTP persistent connection ）之后的好处，包括可以使用HTTP 流水线技术（HTTP pipelining，也有翻译为管道化连接），它是指，在一个TCP连接内，多个HTTP请求可以并行，下一个HTTP请求在上一个HTTP请求的应答完成之前就发起。从wiki上了解到这个技术目前并没有广泛使用，使用这个技术必须要求客户端和服务器端都能支持，目前有部分浏览器完全支持，而服务端的支持仅需要：按HTTP请求顺序正确返回Response（也就是请求&响应采用FIFO模式），wiki里也特地指出，只要服务器能够正确处理使用HTTP pipelinning的客户端请求，那么服务器就算是支持了HTTP pipelining。
    由于要求服务端返回响应数据的顺序必须跟客户端请求时的顺序一致，这样也就是要求FIFO，这容易导致Head-of-line blocking：第一个请求的响应发送影响到了后边的请求，因为这个原因导致HTTP流水线技术对性能的提升并不明显（wiki提到，这个问题会在HTTP2.0中解决）。另外，使用这个技术的还必须是幂等的HTTP方法，因为客户端无法得知当前已经处理到什么地步，重试后可能发生不可预测的结果。POST方法不是幂等的：同样的报文，第一次POST跟第二次POST在服务端的表现可能会不一样。
    在HTTP长连接的wiki中提到了HTTP1.1的流水线技术对RFC规定一个用户最多两个连接的指导意义：流水线技术实现好了，那么多连接并不能提升性能。我也觉得如此，并发已经在单个连接中实现了，多连接就没啥必要，除非瓶颈在于单个连接上的资源限制迫使不得不多开连接抢资源。
    目前浏览器并不太重视这个技术，毕竟性能提升有限。
--------
Netty服务端：
public static void main(String[] args) {

        NioEventLoopGroup boos   =  new NioEventLoopGroup();
        NioEventLoopGroup wokrer =  new NioEventLoopGroup();

        ServerBootstrap serverBootstrap = new ServerBootstrap();

        serverBootstrap.group(boos,wokrer).channel(NioServerSocketChannel.class).childHandler(new ChannelInitializer<NioSocketChannel>() {
                    protected void initChannel(NioSocketChannel ch){
                        ch.pipeline().addLast(new StringDecoder());
                        ch.pipeline().addLast(new SimpleChannelInboundHandler<String>() {
                            @Override
                            protected void channelRead0(ChannelHandlerContext channelHandlerContext, String s) throws Exception {
                                System.out.println(s);
                            }
                        });
                    }
                }).bind(9522);
    }
    private static void bind(final ServerBootstrap serverBootstrap,final int port){
        serverBootstrap.bind(port).addListener(new GenericFutureListener<Future<? super Void>>() {
            @Override
            public void operationComplete(Future<? super Void> future) throws Exception {
                if (future.isSuccess()){
                    System.out.println("端口[" + port + "]绑定成功!");
                } else {
                    System.out.println("端口[" + port + "]绑定失败!");
                    bind(serverBootstrap,port+1);
                }
            }
        });
    }
首先，我们创建了两个NioEventLoopGroup，这两个对象可以看做是传统的IO编程模型的两大线程组，boos表示监听端口，accept新连接的线程组，worker表示处理每一条连接数据读写的线程组。用生活中的例子来讲就是，一个工厂要运作，必然要有一个老板负责从外面接活，然后有很多员工，负责具体干活，老板就是bossGroup，员工们就是workerGroup,bossGroup接收完连接，扔给workerGroup去处理。
接下来，创建一个引导类ServerBootstrap，这个类将引导我们进行服务端的启动工作。然后通过.group(boos,worker)给引导类配置两大线程组，这个引导类的线程模型也就定型了。然后通过.channel(NioServerSocketChannel.class)指定服务端的IO模型为NIO。当然，这里也有其他的选择，如果你想指定 IO 模型为 BIO，那么这里配置上OioServerSocketChannel.class类型即可，当然通常我们也不会这么做，因为Netty的优势就在于NIO。接着，调用childHandler()方法，给这个引导类创建一个ChannelInitializer，这主要是定义后续每条连接的数据读写，业务处理逻辑。ChannelInitializer这个类中，有一个泛型参数NioSocketChannel,这个类就是Netty对NIO类型连接的抽象类，我们前面NioServerScoketChannel也是对NIO类型的连接的抽象，NioServerSocketChannel和NioSocketChannel的概念可以和BIO编程模型中的ServerSocket以及Socket两个概念对应上。总结一下就是，要启动一个Netty服务端，必须要指定三类属性，分别是线程模型、IO 模型、连接读写处理逻辑，有了这三者，之后在调用bind(8000)，我们就可以在本地绑定一个 8000 端口启动起来。

serverBootstrap.bind(8000).addListener(new GenericFutureListener<Future<? super Void>>() {
    public void operationComplete(Future<? super Void> future) {
        if (future.isSuccess()) {
            System.out.println("端口绑定成功!");
        } else {
            System.err.println("端口绑定失败!");
        }
    }
});
serverBootstrap.bind(8000);这个方法呢，它是一个异步的方法，调用之后是立即返回的，他的返回值是一个ChannelFuture，我们可以给这个ChannelFuture添加一个监听器GenericFutureListener，然后我们在GenericFutureListener的operationComplete方法里面，我们可以监听端口是否绑定成功。
总结：
Netty 服务端启动的流程，一句话来说就是：创建一个引导类，然后给他指定 线程模型，IO模型，连接 读写处理逻辑，绑定端口之后，服务端就启动起来了。
然后，我们学习到 bind 方法是异步的，我们可以通过这个异步机制来实现端口递增绑定。
最后呢，我们讨论了 Netty 服务端启动额外的参数，主要包括给服务端 Channel 或者客户端 Channel 设置属性值，设置底层 TCP 参数。
从handler，childHandler，attr，childAttr可以看出：child是针对连接的，反之是针对服务器的
所以可以这么理解：服务器like a mother,每条连接都是她的child

************************
Netty客户端：
 public static void main(String[] args) {
        NioEventLoopGroup workerGroup = new NioEventLoopGroup();
        
        Bootstrap bootstrap = new Bootstrap();
        bootstrap
                // 1.指定线程模型
                .group(workerGroup)
                // 2.指定 IO 类型为 NIO
                .channel(NioSocketChannel.class)
                // 3.IO 处理逻辑
                .handler(new ChannelInitializer<SocketChannel>() {
                    @Override
                    public void initChannel(SocketChannel ch) {
                    }
                });
        // 4.建立连接
        bootstrap.connect("juejin.im", 80).addListener(future -> {
            if (future.isSuccess()) {
                System.out.println("连接成功!");
            } else {
                System.err.println("连接失败!");
            }

        });
    }
客户端的启动和服务端很相似，依然需要线程模型，IO模型，以及 IO业务处理逻辑 三大参数，客户端启动的引导类是Bootstrap，负责启动客户端以及连接服务端，而服务端的辅导类是ServerBootstrap，引导类创建完成后，接下来描述一下客户端启动的流程：首先，需要给它指定线程模型，驱动着连接的数据读写；然后指定IO模型为 NioSocketChannel，表示IO模型为NIO，也可以设置IO 模型为 OioSocketChannel，但是通常不会这么做，因为 Netty 的优势在于 NIO；接着，给引导类指定一个handler，这里主要就是定义 连接的 业务处理逻辑。配置完线程模型，IO模型，连接中的业务处理逻辑之后，调用connect方法进行连接，bootstrap.connect(Host,Port)，connect 方法有两个参数，第一个参数可以填写 IP 或者域名，第二个参数填写的是端口号。由于connect方法返回的是一个future，也就是说这个方法是异步的，可以通过addListener方法可以监听到连接是否成功，进而打印出连接信息。
失败重连：
当客户端连接失败的情况下，尝试重新连接，重新连接的逻辑卸载连接失败的逻辑块里；重新连接的时候，依然是调用一样的逻辑，因此，在重连失败的时候递归调用自身。但是，通常情况下，连接建立失败不会立即重新连接，而是会通过一个指数退避的方式，比如每隔 1 秒、2 秒、4 秒、8 秒，以 2 的幂次来建立连接，然后到达一定次数之后就放弃连接，接下来我们就来实现一下这段逻辑，我们默认重试 5 次

connect(bootstrap, "127.0.0.1", 8000, MAX_RETRY);

private static void connect(Bootstrap bootstrap, String host, int port, int retry) {
    bootstrap.connect(host, port).addListener(future -> {
        if (future.isSuccess()) {
            System.out.println("连接成功!");
        } else if (retry == 0) {
            System.err.println("重试次数已用完，放弃连接！");
        } else {
            // 第几次重连
            int order = (MAX_RETRY - retry) + 1;
            // 本次重连的间隔
            int delay = 1 << order;
            System.err.println(new Date() + ": 连接失败，第" + order + "次重连……");
            bootstrap.config().group().schedule(() -> connect(bootstrap, host, port, retry - 1), delay, TimeUnit
                    .SECONDS);
        }
    });
}
从上面的代码可以看到，通过判断连接是否成功以及剩余重试次数，分别执行不同的逻辑.
1.如果连接成功则打印连接成功的消息.
2.如果连接失败但是重试次数已经用完，放弃连接.
3.如果连接失败但是重试次数仍然没有用完，则计算下一次重连间隔 delay，然后定期重连.

上面的代码中，定时任务是调用bootstrap.config().group().schedule()，其中bootstrap.config()返回的是BootstrapConfig，他是对Bootstrap配置参数的抽象，然后.group()返回的就是我们在一开始的时候配置的线程模型workerGroup，调用workerGroup的schedule方法即可实现定时任务逻辑。
在schedule方法块里，前面四个参数原封不动的传递，最后一个重试次数减掉一。
总结
首先是 Netty 客户端启动的流程，一句话来说就是：创建一个引导类，然后给他指定线程模型，IO 模型，连接 读写处理逻辑，连接上特定主机和端口，客户端就启动起来了。
然后，学习到 connect 方法是异步的，我们可以通过这个异步回调机制来实现指数退避重连逻辑。
最后呢，讨论了 Netty 客户端启动额外的参数，主要包括给客户端 Channel 绑定自定义属性值，设置底层 TCP 参数，相比于服务端少了child的方法，因为客户端发的就是一条连接。
************************
客户端与服务端的双向通信：
客户端发送数据：
客户端相关数据读写逻辑是通过Bootstrap的handler()方法指定的，然后在客户端的initChannel()方法里面给客户端添加一个逻辑处理器，这个处理器的作用就是负责向服务端写数据
.handler(new ChannelInitializer<SocketChannel>() {
    @Override
    public void initChannel(SocketChannel ch) {
        ch.pipeline().addLast(new FirstClientHandler());
    }
});
ch.pipeline()返回的是和这条连接相关的逻辑处理链，采用了责任链模式(分级处理，当前对象处理不了的话，传递给它的上一级处理)，然后调用addLast()方法，添加一个逻辑处理器，这个逻辑处理器为的就是在客户端建立连接成功之后，向服务端写数据，下面就是这个逻辑处理器相关的代码
public class FirstClientHandler extends ChannelInboundHandlerAdapter {
    @Override
    public void channelActive(ChannelHandlerContext ctx) {
        System.out.println(new Date() + ": 客户端写出数据");

        // 1. 获取数据
        ByteBuf buffer = getByteBuf(ctx);

        // 2. 写数据
        ctx.channel().writeAndFlush(buffer);
    }

    private ByteBuf getByteBuf(ChannelHandlerContext ctx) {
        // 1. 获取二进制抽象 ByteBuf
        ByteBuf buffer = ctx.alloc().buffer();
        
        // 2. 准备数据，指定字符串的字符集为 utf-8
        byte[] bytes = "你好，闪电侠!".getBytes(Charset.forName("utf-8"));

        // 3. 填充数据到 ByteBuf
        buffer.writeBytes(bytes);

        return buffer;
    }
}
这个逻辑处理器继承自ChannelInboundHandlerAdapter，然后覆盖了ChannelActive()方法，这个方法会在客户连接建立成功之后被调用，我们将 向服务端写数据的逻辑 写在这个方法里。写数据的逻辑分为两步：首先需要获取一个netty对二进制数据的抽象ByteBuf，ctx.alloc()获取到一个ByteBuf的内存管理器，这个内存管理器的作用就是分配一个ByteBuf，然后我们将字符串的二进制数据填充到ByteBuf，这样就获取到了Netty需要的一个数据格式，最后调用ctx.channel().writeAndFlush()把数据写到服务端。Netty 里面数据是以 ByteBuf 为单位的， 所有需要写出的数据都必须塞到一个 ByteBuf，数据的写出是如此，数据的读取亦是如此。
服务端读取数据：
服务端的相关数据处理逻辑是通过ServerBootstrap的childHandler()方法指定，现在需要在initChannel()方法里给服务端添加一个逻辑处理器，用这个处理器来读取客户端来的数据
.childHandler(new ChannelInitializer<NioSocketChannel>() {
   protected void initChannel(NioSocketChannel ch) {
       // 指定连接数据读写逻辑
	ch.pipeline().addLast(new FirstServerHandler());
   }
});
这个方法里的逻辑和客户端侧类似，获取服务端侧关于这条连接的逻辑处理链pipeline，然后添加一个逻辑处理器，负责读取客户端来的数据
public class FirstServerHandler extends ChannelInboundHandlerAdapter {
    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) {
        ByteBuf byteBuf = (ByteBuf) msg;
        System.out.println(new Date() + ": 服务端读到数据 -> " + byteBuf.toString(Charset.forName("utf-8")));
    }
}
服务端侧的逻辑处理器同样继承自 ChannelInboundHandlerAdapter，与客户端不同的是，这里覆盖的方法是 channelRead()，这个方法在接收到客户端发来的数据之后被回调。
这里的 msg 参数指的就是 Netty 里面数据读写的载体，为什么这里不直接是 ByteBuf，而需要我们强转一下，我们后面会分析到。这里我们强转之后，然后调用 byteBuf.toString() 就能够拿到我们客户端发过来的字符串数据。
服务端回数据给客户端：
服务端向客户端写数据逻辑与客户端侧的写数据逻辑一样，先创建一个ByteBuf，然后填充二进制数据，最后调用writerAndFlush()方法写出去
public class FirstServerHandler extends ChannelInboundHandlerAdapter {
    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
        //读数据代码省略……
        //服务端回数据
        System.out.println(new Date() + ": 服务端写出数据");
        ByteBuf out = getByteBuf(ctx);
        ctx.channel().writeAndFlush(out);
    }

    private ByteBuf getByteBuf(ChannelHandlerContext ctx) {
        byte[] bytes = "服务端回传给客户端的数据kkkkkkkk".getBytes(Charset.forName("utf-8"));
        ByteBuf buffer = ctx.alloc().buffer();
        buffer.writeBytes(bytes);
        return buffer;
    }
}
然后，轮到客户端读取数据了。客户端的读取数据的逻辑和服务端读取数据的逻辑一样，同样是覆盖ChannelRead()方法
 @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) {
        ByteBuf byteBuf = (ByteBuf) msg;
        System.out.println(new Date() + ": 客户端读到数据 -> " + byteBuf.toString(Charset.forName("utf-8")));
    }
总结：
客户端与服务端的逻辑处理均是在启动的时候，通过给逻辑处理链pipeline添加逻辑处理器，来编写数据的读写逻辑；接下来，在客户端连接成功之后会回调到逻辑处理器的channelActive()方法，而不管是服务端还是客户端，收到数据后都会调用channelRead()方法；写数据调用writeAndFlush()方法，客户端与服务端交互的二进制数据载体为ByteBuf，ByteBuf通过连接的内存管理器创建，字节数据填充到ByteBuf后才能写到对端。
************************
数据传输载体--ByteBuf介绍：在Netty中，数据的读写，是以ByteBuf为单位进行交互的，Netty关于数据读写只认ByteBuf

ByteBuf的结构：
1.ByteBuf是一个字节容器，有三部分，第一部分是已丢弃的字节、这部分数据是无效的；第二部分是可读字节，这部分数据是ByteBuf的主体数据，从ByteBuf里面读取到的数据都来自这一部分；最后一部分是可写字节，所有写到ByteBuf的数据都会写到这一段。最后还有一部分是表示ByteBuf最多还能扩容多少容量。
2.以上三部分内容被两个指针给划分出来，从左到右，依次是读指针(readerindex)，写指针(writerindex)，然后还有一个表示ByteBuf底层内存总容量的 变量capacity。
3.从ByteBuf中每读取一个字节，读指针都会自增1，ByteBuf里一共有 writerindex-readerindex 个字节可读，所以当writeindex=readerindex 的时候，ByteBuf不可读。
4.写数据也是从 writerindex 指向的部分开始写的。每写一个字节，writerindex自增1，直到增到capacity时，表示ByteBuf不可写了。
5.ByteBuf里其实还有一个参数maxCapacity，当向ByteBuf中写数据，内存不足时，就可以进行扩容，直到Capacity扩容到maxCapacity，超过maxCapacity就会报错。
Netty 使用 ByteBuf 这个数据结构可以有效地区分可读数据和可写数据，读写之间相互没有冲突，当然，ByteBuf 只是对二进制数据的抽象.

ByteBuf的API：

容量 API：
capacity()：
表示 ByteBuf 底层占用了多少字节的内存（包括丢弃的字节、可读字节、可写字节），不同的底层实现机制有不同的计算方式，后面我们讲 ByteBuf 的分类的时候会讲到。
maxCapacity()：
表示 ByteBuf 底层最大能够占用多少字节的内存，当向 ByteBuf 中写数据的时候，如果发现容量不足，则进行扩容，直到扩容到 maxCapacity，超过这个数，就抛异常。
readableBytes() 与 isReadable()：
readableBytes() 表示 ByteBuf 当前可读的字节数，它的值等于 writerIndex-readerIndex，如果两者相等，则不可读，isReadable() 方法返回 false。
writableBytes()、 isWritable() 与 maxWritableBytes()：
writableBytes() 表示 ByteBuf 当前可写的字节数，它的值等于 capacity-writerIndex，如果两者相等，则表示不可写，isWritable() 返回 false，但是这个时候，并不代表不能往 ByteBuf 中写数据了， 如果发现往 ByteBuf 中写数据写不进去的话，Netty 会自动扩容 ByteBuf，直到扩容到底层的内存大小为 maxCapacity，而 maxWritableBytes() 就表示可写的最大字节数，它的值等于 maxCapacity-writerIndex。

读写 指针 相关API：

readerIndex() 与 readerIndex(int)
前者表示返回当前的读指针 readerIndex, 后者表示设置读指针。
writeIndex() 与 writeIndex(int) 同上：写指针。

markReaderIndex() 与 resetReaderIndex()
前者表示将当前的读指针保存起来，后者表示将当前的读指针恢复到之前保存的值，这两个方法对应使用；
markWriterIndex() 与 resetWriterIndex() 同上：写指针。
上面两组(四个)方法，等价于上面第一组方法 int i = buffer.rederIndex(); ByteBuf b =  buffer.readerIndex(i);不过，推荐使用buffer.markReaderIndex() 和 buffer.resetReaderIndex();不需要自己定义变量，无论buffer当做参数传递到哪里，调用resetReaderIndex()都可以恢复到之前的状态，在解析自定义协议的数据包的时候非常常见。

读写 API：本质上，关于ByteBuf的读写，都可以看作从指针开始的地方开始读写数据。

writeBytes(byte[] var1) 与 buffer.readBytes(byte[] var2):
writeBytes(byte[] var1)表示把 字节数组var1 里的值全部写到ByteBuf，而buffer.readBytes(byte[] var2)表示把ByteBuf里面的数据全部读取到 字节数组var2 里，这里var2的大小通常等于readableBytes(),而var1的大小长度通常小于等于writableBytes()。
writeByte(byte b) 与 buffer.readByte()：
前者表示往ByteBuf中写一个字节，而后者表示从ByteBuf中读取一个字节，类似的API还有writeBoolean()、writeChar()、writeShort()、writeInt()、writeLong()、writeFloat()、writeDouble() 与 readBoolean()、readChar()、readShort()、readInt()、readLong()、readFloat()、readDouble()等；
与读写API类似的API还有getBytes()、getByte()和setBytes()、setByte()系列，唯一的区别就是get/set不会改变读写指针，而read/write会改变读写指针。
release() 与 retain()：
由于Netty使用了堆外内存，而堆外内存不被JVM直接管理，也就是说申请到的内存无法被垃圾回收器直接回收，需要我们手动回收。Netty的ByteBuf是通过引用计数的方式管理的，如果一个ByteBuf没有地方被引用到，需要回收底层内存。默认情况下，当创建完一个ByteBuf，它的引用为1，然后每次调用retain()方法，它的引用都会加1，release()方法原理是将引用计数减1，减完之后如果发现引用计数为0，则直接回收ByteBuf底层的内存。

slice()、duplicate()、copy()：
1.slice()方法是从原始的ByteBuf中截取一段，这段数据是从 读指针(readerIndex) 到 写指针(writerIndex)，同时，返回的截取出来的新的 ByteBuf的 最大容量 maxCapacity 为原始ByteBuf 的 readableBytes(),也就是只有 可读字节区域 的数据(不可写的)。
2.duplicate()方法是把整个ByteBuf都截取出来，包括所有的数据，指针信息。
3.slice()方法与duplicate()方法的相同点是：底层内存 以及 引用计数 与 原始的ByteBuf 共享，也就是说经过slice()或者 duplicate()返回的ByteBuf调用write系列方法 都会影响到 原始的ByteBuf，但是它们都维持着与原始ByteBuf 相同的 内存引用计数 和 不同的读写指针。
4.slice()方法 与 duplicate()不同点就是：slice()只截取从readerIndex到writeIndex之间的数据，它返回的ByteBuf的最大容量被限制到原始的ByteBuf的readableBytes()，而duplicate是把 整个ByteBuf都与原始的ByteBuf共享。
5.slice()方法与duplicate()方法 不会拷贝数据，它们值是通过改变读写指针来改变读写的行为，而最后一个方法 capy()会直接从原始的ByteBuf中拷贝所有的信息，包括读写指针以及底层对应的数据，因此，往copy()方法返回的数据中写数据，不会影响到原始的ByteBuf。
6.slice()和duplicate()方法不会改变ByteBuf的引用计数，所以原始的ByteBuf调用release()之后，如果发现引用计数为0，就开始释放内存，通过这两个方法返回的ByteBuf也会被释放，如果这个时候再对它们进行读写操作，就会报错。因此，在调用这两个方法的时候，同时我们可以通过retain()方法来增加引用，表示它们对应的底层内存多了一层引用，引用计数为2， 在释放内存时，需要调用两次release()方法，将引用计数降到0，才会释放内存。
7.这三个方法均维护着自己的读写指针，与原始的ByteBuf的读写指针无关，相互之间不受影响。
---评论区摘抄：
简短概括：copy和原 ByteBuf完全是不同的底层数组，读写都不影响；slice和duplicate和原 ByteBuf之间，读互相不影响；slice不能写，duplicate和原 ByteBuf中若有一个在写，必会影响另一个。所以duplicate出来的 ByteBuf是不是最好不要写？

retainedSlice() 与 retainedDuplicate()：
这两个方法的作用是在截取内存片段的同时，增加内存的引用计数，分别等价于以下两段代码： slice.retain(); duplicate.retain();
使用到slice和duplicate方法的时候，千万要清理内存共享，引用计数共享，读写指针不共享几个概念，下面举两个常见易犯错的例子。
1.多次释放：
Buffer buffer = xxx;
doWith(buffer);
// 一次释放
buffer.release();

public void doWith(Bytebuf buffer) {
// ...    
    
// 没有增加引用计数
Buffer slice = buffer.slice();

foo(slice);

}

public void foo(ByteBuf buffer) {
    // read from buffer
    
    // 重复释放
    buffer.release();
}
这里的 doWith 有的时候是用户自定义的方法，有的时候是 Netty 的回调方法，比如 channelRead() 等等

2.不释放造成内存泄漏：
Buffer buffer = xxx;
doWith(buffer);
// 引用计数为2，调用 release 方法之后，引用计数为1，无法释放内存 
buffer.release();

public void doWith(Bytebuf buffer) {
// ...    
    
// 增加引用计数
Buffer slice = buffer.retainedSlice();

foo(slice);

// 没有调用 release

}

public void foo(ByteBuf buffer) {
    // read from buffer
}
想要避免以上两种情况发生，只需要记得一点，在一个函数体里面，只要增加了引用计数（包括 ByteBuf 的创建和手动调用 retain() 方法），就必须调用 release() 方法。

实战Demo：
public class ByteBufTest {
    public static void main(String[] args) {
        ByteBuf buffer = ByteBufAllocator.DEFAULT.buffer(9, 100);

        print("allocate ByteBuf(9, 100)", buffer);

        // write 方法改变写指针，写完之后写指针未到 capacity 的时候，buffer 仍然可写
        buffer.writeBytes(new byte[]{1, 2, 3, 4});
        print("writeBytes(1,2,3,4)", buffer);

        // write 方法改变写指针，写完之后写指针未到 capacity 的时候，buffer 仍然可写, 写完 int 类型之后，写指针增加4
        buffer.writeInt(12);
        print("writeInt(12)", buffer);

        // write 方法改变写指针, 写完之后写指针等于 capacity 的时候，buffer 不可写
        buffer.writeBytes(new byte[]{5});
        print("writeBytes(5)", buffer);

        // write 方法改变写指针，写的时候发现 buffer 不可写则开始扩容，扩容之后 capacity 随即改变
        buffer.writeBytes(new byte[]{6});
        print("writeBytes(6)", buffer);

        // get 方法不改变读写指针
        System.out.println("getByte(3) return: " + buffer.getByte(3));
        System.out.println("getShort(3) return: " + buffer.getShort(3));
        System.out.println("getInt(3) return: " + buffer.getInt(3));
        print("getByte()", buffer);


        // set 方法不改变读写指针
        buffer.setByte(buffer.readableBytes() + 1, 0);
        print("setByte()", buffer);

        // read 方法改变读指针
        byte[] dst = new byte[buffer.readableBytes()];
        buffer.readBytes(dst);
        print("readBytes(" + dst.length + ")", buffer);

    }

    private static void print(String action, ByteBuf buffer) {
        System.out.println("after ===========" + action + "============");
        System.out.println("capacity(): " + buffer.capacity());
        System.out.println("maxCapacity(): " + buffer.maxCapacity());
        System.out.println("readerIndex(): " + buffer.readerIndex());
        System.out.println("readableBytes(): " + buffer.readableBytes());
        System.out.println("isReadable(): " + buffer.isReadable());
        System.out.println("writerIndex(): " + buffer.writerIndex());
        System.out.println("writableBytes(): " + buffer.writableBytes());
        System.out.println("isWritable(): " + buffer.isWritable());
        System.out.println("maxWritableBytes(): " + buffer.maxWritableBytes());
        System.out.println();
    }
}

总结：
1.首先分析了Netty是对二进制数据的抽象ByteBuf的结构，本质上它的原理就是，它引用了一段内存，这段内存可以是堆内的也可以是堆外的，然后引用计数来控制这段内存是否需要被释放，使用读写指针来控制对ByteBuf的读写，可以理解是外观模式的一种使用。
2.基于读写指针和容量、最大可扩容容量，衍生出一系列的读写方法，要注意 read/write(会改变读写指针) 与 get/set(不会改变读写指针) 的区别。
3.多个ByteBuf可以引用同一段内存，通过引用计数来控制内存的释放，遵循谁retain() 谁 release()的原则。
************************
客户端与服务端客户协议编解码：
无论是Netty还是原始的socket编程，基于TCP通信的数据包格式均为二进制，协议指的就是客户端与服务端事先商量好的，每一个二进制数据包中每一段字节分别代表什么含义的规则。
客户端与服务端通信过程：客户端将一个Java对象按照通信协议转换成二进制的数据包，然后通过网络，把这个二进制数据包传递到客户端，数据的传输过程由TCP/IP协议负责数据的传输，与应用层无关。服务端接收到数据后，按照通信协议取出二进制数据包中的相应字段，包装成Java对象，交给 应用逻辑 处理。服务端处理完后，如果需要突出响应给客户端，那么按照相同的流程进行处理。
通信协议的设计：
1.首先，第一个字段是 魔数 ，通常情况下为固定的几个字节(这边先规定4个字节)，服务端在接收到数据包后，会首先取出前面的 几个字节 和 魔数 进行比对，能够在第一时间辨别出这个数据包是否遵循自定义协议，如果识别出这个数据包并非是遵循自定义协议的，也就是无效数据包，为了安全考虑可以直接关闭连接以节省资源。在 Java 的字节码的二进制文件中，开头的 4 个字节为0xcafebabe 用来标识这是个字节码文件，亦是异曲同工之妙。
2.接下来 一个字节 为版本号，通常情况下是预留字段，用于协议升级的时候用的，有点类似TCP协议中的一个字段表示 是IPV4还是IPV6协议，大多数情况下，这个字段是用不到的，不过为了协议能够支持升级，还是先留着。
3.第三部分，序列化算法表示如何把Java对象转换二进制数据以及二进制数据如何转换回Java对象，比如 Java 自带的序列化，json，hessian等序列化方式。
4.第四部分的字段表示指令，客户端或服务端每接受到一种指令，就会有相应的处理逻辑，这里用一个字节(占八位)来表示，最高支持256种指令，对于这个要设计的IM系统来说已经完全足够了。
5.第五部分为数据的长度，占四个字节。
通信协议的实现：
将Java对象根据协议封装成二进制数据包的过程称为编码，将从二进制数据包中解析出Java对象的过程叫做解码。
Java对象抽象类：版本协议号，以及获取 操作指令 的方法，所有的指令数据包必须实现这个方法，这样我们就可以知道某种指令的含义。
@Data
public abstract class Packet {
    /* 协议版本*/
    private Byte version = 1;
    /* 指令 /
    public abstract Byte getCommand();
}
以客户端登录请求为例，定义登录请求数据包：
public interface Command {
//在指令接口里定义值
    Byte LOGIN_REQUEST = 1;
}

@Data
public class LoginRequestPacket extends Packet {
//定义字段，用户id，用户名，密码
    private Integer userId;

    private String username;

    private String password;

    @Override
    public Byte getCommand() {
        //重写获取指令方法
        return LOGIN_REQUEST;
    }
}
接下来定义一种规则，将Java对象转换成二进制数据，这个规则叫做Java对象的序列化：
序列化接口:
public interface Serializer {

    /**
     * 获取序列化算法
     */
    byte getSerializerAlgorithm();
    
    /**
     * java 对象转换成二进制
     */
    byte[] serialize(Object object);

    /**
     * 二进制转换成 java 对象
     */
    <T> T deserialize(Class<T> clazz, byte[] bytes);
}
例子中，使用最简单的json序列化方式，并使用阿里巴巴的fastjson作为序列化框架。
public interface SerializerAlgorithm {
    /**
     * json 序列化标识
     */
    byte JSON = 1;

}


public class JSONSerializer implements Serializer {
   
    @Override
    public byte getSerializerAlgorithm() {
        
        return SerializerAlgorithm.JSON;
    } 

    @Override
    public byte[] serialize(Object object) {
        
        return JSON.toJSONBytes(object);
    }

    @Override
    public <T> T deserialize(Class<T> clazz, byte[] bytes) {
        
        return JSON.parseObject(bytes, clazz);
    }
}
定义 序列化算法的类型 以及 默认的序列化算法
public interface SerializerAlgorithm {
    /*
    * json序列化标识
    * */
    byte JSON = 1;

    /*
     * protobuf序列化标识
     * */
    byte PROTOBUF = 2;

}
public interface Serializer {

   Serializer DEFAULT = new JSONSerializer();
   //..............省略方法声明
｝
封装成二进制数据：
private static final int MAGIC_NUMBER = 0x12345678;

public ByteBuf encode(Packet packet) {
    // 1. 创建 ByteBuf 对象
    ByteBuf byteBuf = ByteBufAllocator.DEFAULT.ioBuffer();
    // 2. 序列化 Java 对象
    byte[] bytes = Serializer.DEFAULT.serialize(packet);

    // 3. 实际编码过程
    byteBuf.writeInt(MAGIC_NUMBER);
    byteBuf.writeByte(packet.getVersion());
    byteBuf.writeByte(Serializer.DEFAULT.getSerializerAlgorithm());
    byteBuf.writeByte(packet.getCommand());
    byteBuf.writeInt(bytes.length);
    byteBuf.writeBytes(bytes);

    return byteBuf;
}
编码过程分为三个过程：
1.首先，创建一个ByteBuf，这里可以调用Netty的ByteBuf分配器来创建，ioBuffer()方法会返回适配io读写相关的内存，它会尽可能的创建一个直接内存(堆外内存)，直接内存可以理解为不受jvm堆管理的内存空间，写到IO缓冲区的效果更高。
2.然后，将Java对象序列化成二进制数据包。
3.最后，根据之前 设计的协议，逐个往ByteBuf中写入字段，即实现了编码过程。
一端实现了编码之后，Netty会将此ByteBuf写到另外一端，另外一端拿到的也是ByteBuf对象，基于这个ByteBuf对象，就可以在对端反解创建出Java对象，这个过程称为解码。
解码：解析Java对象的过程
public Packet decode(ByteBuf byteBuf) {
    // 跳过 magic number
    byteBuf.skipBytes(4);

    // 跳过版本号
    byteBuf.skipBytes(1);

    // 序列化算法标识
    byte serializeAlgorithm = byteBuf.readByte();

    // 指令
    byte command = byteBuf.readByte();

    // 数据包长度
    int length = byteBuf.readInt();

    byte[] bytes = new byte[length];
    byteBuf.readBytes(bytes);

    Class<? extends Packet> requestType = getRequestType(command);

    Serializer serializer = getSerializer(serializeAlgorithm);

    if (requestType != null && serializer != null) {
        return serializer.deserialize(requestType, bytes);
    }
    return null;
} 
    private Class getRequestType(Byte command){
        return packetTypeMap.get(command);
    }
    private Serializer getSerializer(Byte serializeAlgorithm){
        return serializerMap.get(serializeAlgorithm);
    }
解码的流程：
1.假定decod方法传递寄来的ByteBuf是合法的，即 我们可以跳过魔数的验证(调用skipBytes，跳过前4个字节)、
2.这里，我们暂不关注协议版本，所以也可以跳过版本号信息(调用skipBytes，跳过1个字节)
3.然后，调用ByteBuf的API分别拿到 序列化算法标识，指令，数据包长度。
4.最后，根据拿到的数据包的长度，取出数据，通过 指令 拿到该 数据包对应的Java对象类型 ，根据序列化算法标识 拿到序列化对象，将字节数组转换为Java对象，至此，解码过程结束。
可以看到，编码与解码是一个相反的过程

总结：
1.通信协议是为了满足服务端与客户端之间的交互，协商出来的满足一定规则的二进制数据格式。
2.介绍了一种通用的通信协议的设计，包括了 魔数、版本号、序列化算法标识、指令、数据长度、数据内容 几个字段，该协议满足绝大多数的通信场景。
3.Java对象以及序列化，目的就是实现Java对象和二进制数据之间的转化。
4.最后依照设计的协议以及ByteBuf的API实现了通信协议，这个过程称为编解码过程。
问：
序列化和编码都是把 Java 对象封装成二进制数据的过程，这两者有什么区别和联系？
序列化是把内容变成计算机可传输的资源，而编码则是让程序认识这份资源。
************************
实战：实现客户端登陆
登陆的逻辑：
1.客户端会构建一个登录请求对象，然后通过 编码 把请求对象编码为ByteBuf，然后写到服务端。
2.然后服务端通过读取操作，读到ByteBuf之后，用 解码操作 把ByteBuf解码成 登录请求对象，然后对其中的数据进行校验。
3.客户端校验通过之后，构造一个 登陆响应对象，依然经过编码，编码成ByteBuf，然后再写回到客户端。
4.客户端接收到服务端的ByteBuf之后，将其 解码，拿到 登陆响应对象 的结果，通过读取其中的数据，判断登陆是否成功。
逻辑处理器:
首先，在客户端引导类Bootstrap.handler(new ChannelInitializer<SocketChannel>)的重写方法initChannel()中配置客户端的处理逻辑ch.piepline().addLast(new 逻辑处理器类),定义逻辑处理链器类 ClientHandler extends ChannelInboundHandlerAdapter,同样，在服务端定义引导类Bootstrap.childHandler(new ChannelInitializer<NioSocketChannel>)的重写方法initChannel()中配置服务端的处理逻辑ch.pieline()/addLast(new 逻辑处理器类)，定义处理逻辑器类 ServerHandler extends ChannelInboundHandlerAdapter。
@override				          @override
channelActive()在客户端连接建立成功之后被调用   channelRead() 服务端在接收到客户端(另一端)发过来的数据后被回调
总结:
梳理了一下客户端登录的基本流程，然后结合编解码逻辑，使用 Netty 实现了完整的客户端登录流程。
************************
实战：实现客户端与服务端收发消息
判断客户端是否登录成功：
首先，定义MessageRequestPacket消息请求数据包和MessageResponsePacket消息响应数据包，在指令中添加两个新的指令，然后定义新的类，存放用于标记是否登录和判断是否登录的方法（用连接的attr属性存放标记）。并在客户端收到登录成功的响应时，调用标记登录的方法，将该链接标记为已登录。
判断客户端是否登录成功：
我们可以给 客户端连接Channel绑定属性，通过channel.attr(  xxx  该位置要存放AttrbuteKey类型  ).set( xxx 可以在定义AttrbuteKey<>的时候定义泛型,该位置存放泛型类型的数据 )的方式，那么我们登录成功之后，给Channel绑定一个登录成功的标识，然后判断是否登录成功的时候，通过这个标识位来判断就可以了。
public interface Attributes {
    AttributeKey<Boolean> LOGIN = AttributeKey.newInstance("login");
}

public class LoginUtil {
    public static void markAsLogin(Channel channel) {
        channel.attr(Attributes.LOGIN).set(true);
    }

    public static boolean hasLogin(Channel channel) {
        Attribute<Boolean> loginAttr = channel.attr(Attributes.LOGIN);

        return loginAttr.get() != null;
    }
}
我们抽取出LoginUtil用于设置登录标识位 以及判断是否有标识位，如果有标识位，不管标识位的值是什么，都表示已经成功登陆过。接下来，我们来实现控制台输入消息并发送到客户端。
客户端控制台输入消息并发送：
NettyClient.java


private static void connect(Bootstrap bootstrap, String host, int port, int retry) {
    bootstrap.connect(host, port).addListener(future -> {
        if (future.isSuccess()) {
            Channel channel = ((ChannelFuture) future).channel();
            // 连接成功之后，启动控制台线程
            startConsoleThread(channel);
        } 
        // ...
    });
}

private static void startConsoleThread(Channel channel) {
    new Thread(() -> {
        while (!Thread.interrupted()) {
            if (LoginUtil.hasLogin(channel)) {
                System.out.println("输入消息发送至服务端: ");
                Scanner sc = new Scanner(System.in);
                String line = sc.nextLine();
                
                MessageRequestPacket packet = new MessageRequestPacket();
                packet.setMessage(line);
                ByteBuf byteBuf = PacketCodeC.INSTANCE.encode(channel.alloc(), packet);
                channel.writeAndFlush(byteBuf);
            }
        }
    }).start();
}
连接成功之后，调用自定义的 startConsoleThread()方法，开始启动控制台线程，然后在控制台线程中，判断 只要当前的Channel是登录状态，就允许控制台输入消息。从控制台收到消息后，将消息封装成消息对象，并编码成ByteBuf，最后通过writeAndFlush()，将消息写到服务端。
服务端收发消息处理：
ServerHandler.java

public void channelRead(ChannelHandlerContext ctx, Object msg) {
    ByteBuf requestByteBuf = (ByteBuf) msg;

    Packet packet = PacketCodeC.INSTANCE.decode(requestByteBuf);

    if (packet instanceof LoginRequestPacket) {
        // 处理登录..
    } else if (packet instanceof MessageRequestPacket) {
        // 处理消息
        MessageRequestPacket messageRequestPacket = ((MessageRequestPacket) packet);
        System.out.println(new Date() + ": 收到客户端消息: " + messageRequestPacket.getMessage());

        MessageResponsePacket messageResponsePacket = new MessageResponsePacket();
        messageResponsePacket.setMessage("服务端回复【" + messageRequestPacket.getMessage() + "】");
        ByteBuf responseByteBuf = PacketCodeC.INSTANCE.encode(ctx.alloc(), messageResponsePacket);
        ctx.channel().writeAndFlush(responseByteBuf);
    }
}
服务端收到消息后，仍然回调到channelRead()方法，解码之后用一个else分支进行消息处理。然后，服务端将消息打印到控制台后，封装一个消息响应对象，再将其编码成ByteBuf，然后调用writeAndFlush() 将数据写到客户端。客户端收到服务端的响应消息后，处理方法与之前一样，在ChannelRead()中，用else判断数据类型，完成对消息的处理逻辑。
总结：
1.定义了收发消息的Java对象进行消息的收发。
2.然后学到了channel的attr()的实际用法：可以通过给channel绑定属性来设置某些状态、获取某些状态，不需要Map来维持
3.最后，学习了在控制台获取消息并发送到服务端，然后服务端响应，回复消息给客户端，客户端接收响应 等。
************************
pipeline与channelHandler

像之前，我们把数据流入后的解码、判断数据类型，登录，消息收发、然后将消息编码后进行数据流出的逻辑都写在一个类里面。比如客户端写在了ClientHandler，服务端写在了ServerHandler里，以后如果要进行功能的扩展（比如，我们要校验 魔数，或者其他特殊逻辑），就只能在一个类里面去修改，这个类就会变得越来越臃肿。
另外，我们发现，每次发 指令数据包 都要手动调用编码器编码成 ByteBuf，对于这类场景的编码优化，能想到的办法就是模块化处理，将不同的逻辑放到单独的类中来处理，最后将这些逻辑串联起来，形成一个完整的逻辑处理链。
Netty中的pipeline和ChannelHandler正式来解决这个问题的：他通过 责任链设计模式 来组织代码逻辑，并能够支持动态的添加和删除，Netty还支持各类协议的扩展，比如HTTP，WebSocket，Redis，靠的就是pipeline和channelHandler。

pipeline和ChannelHandler的构成：

无论是从服务端来看，还是客户端来看，在Netty整个框架里面，一条连接对应着一个Channel，这条Channel所有的逻辑处理都在一个叫做Channelpipeline的对象里面，Channelpipeline是一个双向链表结构，他和Channel是一对一的关系。
而Channelpipeline里的每一个节点都是一个ChannelHandlerContext对象，这个对象能够拿到和Channel相关的所有的上下文信息，并且在ChannelHandlerContext中，有一个重要的对象--ChannelHandler。
下面是ChannelHandler的分类：
ChannelHandler有两大子类接口：第一个是ChannelInboundHandler，他是处理读数据的逻辑；第二个是ChannelOutboundHandler，他是处理写数据的逻辑，他是定义我们一段在组装完响应之后，把数据写到对端的逻辑；
这两个子接口分别有对应的默认实现：ChannelInboundHandlerAdapter和ChannelOutboundHandlerAdapter，它们分别实现了两大接口的所有功能，默认情况下会把读写事件传播到下一个handler。

ChannelInboundHandler 的事件传播：

首先，在服务端的pipeline中添加三个ChannelInboundHandler,每个都inBoundHandler 都继承自ChannelInboundHandlerAdapter类,并实现了channelRead()方法，channelRead方法里，打印当前handler的信息，然后调用父类的channelRead方法，父类的channelRead方法会自动调用下一个inBoundHandler 的channelRead方法，并且把inBoundHandler 里处理完毕的对象传递到下一个inBoundHandler 中。我们通过 addLast() 方法来为 pipeline 添加 inBoundHandler，当然，除了这个方法还有其他的方法，感兴趣的同学可以自行浏览一下 pipeline 的 api 。如果在pipeline中添加的ChannelInboundHandler,顺序为ABC，输出执行顺序也是ABC。

ChannelOutboundHandler 的事件传播：

继续在服务端的 pipeline 添加三个 ChanneloutBoundHandler，每个 outBoundHandler 都继承自 ChanneloutBoundHandlerAdapter，然后实现了 write() 方法，剩下的细节和上面的ChannelinBoundHandler事件传播类似，不同的地方是，如果在pipeline中添加的顺序是ABC，那么输出执行的顺序则为CBA。和inbound相反。

总结
通过前面编写客户端服务端处理逻辑，引出了 pipeline 和 channelHandler 的概念。
channelHandler 分为 inBound 和 outBound 两种类型的接口，分别是处理数据读与数据写的逻辑，可与 tcp 协议栈联系起来。
两种类型的 handler 均有相应的默认实现，默认会把事件传递到下一个，这里的传递事件其实说白了就是把本 handler 的处理结果传递到下一个 handler 继续处理。
inBoundHandler 的执行顺序与我们实际的添加顺序相同，而 outBoundHandler 则相反。
************************
实战：构建客户端与服务端 pipeline，把复杂的逻辑从单独的ChannelHandler中抽取出来。
Netty中有很多开箱即食的ChannelHandler，下面我们通过Netty内置的ChannelHandler来逐步构建pipeline

ChannelInboundHandlerAdapter与ChannelOutboundHandlerAdapter：下面简称inbound 和 outbound

首先是ChannelInboundHandlerAdapter 这个适配器主要作用是实现其接口ChannelInboundHandler 的所有方法，这样在编写自己的Handler的时候就不需要实现Handler里的每一个方法了，只需要实现我们所关心的方法，默认情况下，对于ChannelInboundHandlerAdapter，我们比较关心的是他的channelRead方法。
ChannelInboundHandlerAdapter.java
@Override
public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
    ctx.fireChannelRead(msg);
}
它的作用就是接受上一个handler的输出，这里的msg就是上一个handler的输出。默认情况下handler会通过fireChannelRead()方法直接把上一个handler的输出结果传递给下一个handler。
与 ChannelInboundHandlerAdapter 类似的类是 ChannelOutboundHandlerAdapter，它的核心方法如下
ChannelOutboundHandlerAdapter.java
@Override
public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception {
    ctx.write(msg, promise);
}
默认情况下，这个 adapter 也会把对象传递到下一个 outBound 节点，它的传播顺序与 inboundHandler 相反，这里就不再对这个类展开了。
在pipeline中添加的 第一个handler中的 channelRead方法中，msg对象其实就是ByteBuf。服务端在接收到数据后，先解码，然后将解码结果传给下一个handler。
@Override
public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
        ByteBuf requestByteBuf = (ByteBuf) msg;
        // 解码
        Packet packet = PacketCodeC.INSTANCE.decode(requestByteBuf);
        // 解码后的对象传递到下一个 handler 处理
        ctx.fireChannelRead(packet)
}
不过在解码之前。先了解一下另外一个特殊的Handler
ByteToMessageDecoder：
通常情况下，无论是客户端还是服务端，当我们收到数据之后，首先要做的就是把二进制数据转换成对应的Java对象，所以Netty很贴心的写了一个父类ByteToMessageDecoder，专门来做这件事，下面用这个类来实现服务端的解码：
public class PacketDecoder extends ByteToMessageDecoder {
    @Override
    protected void decode(ChannelHandlerContext ctx, ByteBuf in, List out) {
        out.add(PacketCodeC.INSTANCE.decode(in));
    }
}
当我们继承了ByteToMessageDecoder类之后，需要实现它的decode方法，in就是传过来的ByteBuf数据，第三个参数类型是List，通过往这个List里面添加解码后的结果对象，就可以自动实现 结果往下一个 handler中传递，这样就实现了解码过程的handler。
另外，值得注意的一点，对于 Netty 里面的 ByteBuf，我们使用 4.1.6.Final 版本，默认情况下用的是堆外内存，在 ByteBuf 这一小节中我们提到，堆外内存我们需要自行释放(堆内内存由JVM通过GC自动回收)，在我们前面小节的解码的例子中，其实我们已经漏掉了这个操作，这一点是非常致命的，随着程序运行越来越久，内存泄露的问题就慢慢暴露出来了， 而这里我们使用 ByteToMessageDecoder，Netty 会自动进行内存的释放，我们不用操心太多的内存管理方面的逻辑

SimpleChannelInboundHandler：
在之前，我们通过if else 进行逻辑处理，当我们需要处理的指令越来越多的时候，代码就会显得越来越臃肿，我们可以通过给pipeline添加多个handler(ChannelInboundHandlerAdapter的子类)来解决过多if else 的问题，如下：
XXXHandler.java
if ( packet instanceof XXXpacket ){
	//处理...
} else {
	ctx.fireChannelRead(packet);
}
这样有一个好处就是，每次添加一个指令处理器，逻辑处理的框架都是一样的。但是在编写指令处理的handler的时候，依然编写了一段if else判断，然后还要手动传递无法处理的对象（XXXPacket）至下一个指令处理器，这也是一段重复度极高的代码，因此，Netty抽象出了SimpleChannelInboundHandler 对象，类型判断和对象传递的活都自动帮我们实现了，而我们可以专心处理我们的指令即可。
LoginRequestHandler.java

public class LoginRequestHandler extends SimpleChannelInboundHandler<LoginRequestPacket> {
    @Override
    protected void channelRead0(ChannelHandlerContext ctx, LoginRequestPacket loginRequestPacket) {
        // 登录逻辑
    }
}
SimpleChannelInboundHandler从字面意思可以看到，使用他很简单，在继承这个类的时候，给他传递一个泛型参数，然后在channelRead0()方法里，我们不用通过if来判断当前对象是否能交给本handler处理，也不用强转，也不用往下传递本handler处理不了的对象，这一切都交给父类SimpleChannelInboundHandler来实现了，我们只需要专注于我们要处理的业务逻辑即可。
上面的 LoginRequestHandler 是用来处理登录的逻辑，同理，我们可以很轻松地编写一个消息逻辑处理器。
MessageRequestHandler.java

public class MessageRequestHandler extends SimpleChannelInboundHandler<MessageRequestPacket> {
    @Override
    protected void channelRead0(ChannelHandlerContext ctx, MessageRequestPacket messageRequestPacket) {

    }
}
MessageToByteEncoder：
前面实现了登录和消息处理逻辑，处理完请求之后，都会给客户端一个响应，但是在写响应之前，需要把响应对象编码成ByteBuf，最后的逻辑框架如下

public class LoginRequestHandler extends SimpleChannelInboundHandler<LoginRequestPacket> {
    @Override
    protected void channelRead0(ChannelHandlerContext ctx, LoginRequestPacket loginRequestPacket) {
        LoginResponsePacket loginResponsePacket = login(loginRequestPacket);
        ByteBuf responseByteBuf = PacketCodeC.INSTANCE.encode(ctx.alloc(), loginResponsePacket);
        ctx.channel().writeAndFlush(responseByteBuf);
    }
}

public class MessageRequestHandler extends SimpleChannelInboundHandler<MessageRequestPacket> {
    @Override
    protected void channelRead0(ChannelHandlerContext ctx, MessageRequestPacket messageRequestPacket) {
        MessageResponsePacket messageResponsePacket = receiveMessage(messageRequestPacket);
        ByteBuf responseByteBuf = PacketCodeC.INSTANCE.encode(ctx.alloc(), messageRequestPacket);
        ctx.channel().writeAndFlush(responseByteBuf);
    }
}
我们注意到，处理每一种指令完成之后的逻辑都是类似的，都需要进行编码，然后调用writeAndFlush()将数据写到客户端，这个编码的过程其实也是重复的逻辑，而且在编码过程中，还需要我们手动创建一个ByteBuf(PacketCodeC中的自定义encode方法)。而Netty提供了一个特殊的channelHandler来专门处理编码逻辑，我们不需要每一次将相应写到对端的时候调用一次编码逻辑进行编码，也不需要自行创建ByteBuf，这个类就是MessagtToByteEncoder，其功能就是将对象转换到二进制数据。
public class PacketEncoder extends MessageToByteEncoder<Packet> {
    @Override
    protected void encode(ChannelHandlerContext ctx, Packet packet, ByteBuf out) {
        PacketCodeC.INSTANCE.encode(out, packet);
    }
}
这里只需要实现encode()方法，这里面，第二个参数是java对象， 第三个对象是ByteBuf对象，这个方法里要做的事就是将java对象里面的字段写到ByteBuf，而且不需要我们自行去分配ByteBuf，PacketCodeC 的 encode() 方法的定义也改了：
// 更改前的定义
public ByteBuf encode(ByteBufAllocator byteBufAllocator, Packet packet) {
    // 1. 创建 ByteBuf 对象
    ByteBuf byteBuf = byteBufAllocator.ioBuffer();
    // 2. 序列化 java 对象

    // 3. 实际编码过程

    return byteBuf;
}
// 更改后的定义
public void encode(ByteBuf byteBuf, Packet packet) {
    // 1. 序列化 java 对象

    // 2. 实际编码过程
}
packetCodeC中，不再需要手动创建对象， 不再需要把创建完的ByteBuf进行返回。当我们向pipeline中添加了这个编码器之后，我们在指令处理完毕之后就只需要writeAndFlush java对象即可，像这样

public class LoginRequestHandler extends SimpleChannelInboundHandler<LoginRequestPacket> {
    @Override
    protected void channelRead0(ChannelHandlerContext ctx, LoginRequestPacket loginRequestPacket) {
        ctx.channel().writeAndFlush(login(loginRequestPacket));
    }
}

public class MessageRequestHandler extends SimpleChannelInboundHandler<MessageResponsePacket> {
    @Override
    protected void channelRead0(ChannelHandlerContext ctx, MessageResponsePacket messageRequestPacket) {
        ctx.channel().writeAndFlush(receiveMessage(messageRequestPacket));
    }
}
上面分析完服务端的pipeline的组成结构，我们也可以自行分析出客户端的handler结构，最后来看一下服务端和客户端完整的pipeline的handler结构。对应的代码：
服务端：
serverBootstrap
               .childHandler(new ChannelInitializer<NioSocketChannel>() {
                    protected void initChannel(NioSocketChannel ch) {
                        ch.pipeline().addLast(new PacketDecoder());
                        ch.pipeline().addLast(new LoginRequestHandler());
                        ch.pipeline().addLast(new MessageRequestHandler());
                        ch.pipeline().addLast(new PacketEncoder());
                    }
});
客户端：
bootstrap
        .handler(new ChannelInitializer<SocketChannel>() {
            @Override
            public void initChannel(SocketChannel ch) {
                ch.pipeline().addLast(new PacketDecoder());
                ch.pipeline().addLast(new LoginResponseHandler());
                ch.pipeline().addLast(new MessageResponseHandler());
                ch.pipeline().addLast(new PacketEncoder());
            }
});
总结：
通过学习Netty内置的Channelhandler,来逐步构建服务端pipeline，可以减少很多重复逻辑
1.基于ByteToMessageDecoder，可以实现自定义解码，而不用关心ByteBuf的强转和 解码结果 的传递
2.基于SimpleChannelInboundHandler，可以实现每一种指令的处理，不再需要强转，不再有冗长的if else 逻辑，不需要手动传递对象。
ps：在这里出错，墨迹了大半天时间，一直以为是服务端的问题，确实也是服务端的问题，最后发现，服务端的消息请求数据包，没有无参构造，可以加Lomlok的@NoArgsConstructor注解。
************************
实战：拆包粘包理论与解决方案(没有全写，重点都在)
从服务端的控制台输出可以看出，存在三种类型的输出：
一种是正常的字符串输出。
一种是多个字符串“粘”在了一起，我们定义这种 ByteBuf 为 粘包。
一种是一个字符串被“拆”开，形成一个破碎的包，我们定义这种 ByteBuf 为 半包。
为什么会有粘包半包现象？

拆包的原理：在没有Netty的情况下， 用户如果自己需要拆包，基本原理就是不断从TCP缓冲区中读取数据，每次读取完都需要判断是否是一个完整的数据包。
1.如果当前数据不是一个完整的数据包，那就保留该数据，继续从TCP缓冲区中读取数据，直到得到一个完整的数据包。
2.如果当前独到的数据加上已经读取到的数据足够拼接成一个数据包，那就将已经读取的数据拼接上本次读取的数据，构成一个完整的业务数据包传递到业务逻辑，多余的数据仍然保留，以便和下次读到的数据尝试拼接。
如果我们自己实现拆包，这个过程将会非常麻烦，我们每一种自定义的协议，都需要自己实现，还需要考虑各种异常，而Netty自带的一些开箱即食的拆包器，已经完全满足我们的需求了。如下：

Netty自带的拆包器：
1.固定长度的拆包器 FixedLengthFrameDecoder：当应用层协议非常简单，每个数据包的长度都是固定时，那么只需要把这个拆包器加到pipeline中，Netty就会按固定长度把一个个数据包(ByteBuf)传递到下一个channelHandler。
2.行拆包器  LineBasedFrameDecoder：从字面意思来看，每个数据包之前以换行符作为分隔，接收端通过LineBasedFrameDecoder 将粘过来的ByteBuf拆分成一个个完整的应用层数据包。
3.分隔符拆包器 DelimiterBasedFrameDecoder：DelimiterBasedFrameDecoder是行拆包器的通用版本，只不过我们可以自定义分隔符。
4.基于长度域拆包器 LengthFieldBasedFrameDecoder：这种拆包器是最通用的一种拆包器，只要我们自定义的协议中包含长度域字段，均可以使用这个拆包器来实现应用层拆包。	
PS：前三种拆包器比较简单，下面用基于长度的拆包器作为例子来讲解：
如何使用LengthFieldBasedFrameDecoder：
1.首先考虑，在之前我们自定义的协议中，长度域在整个数据包的哪个地方，专业术语来说就是长度域相对整个数据包的偏移量是多少，这里就是 4+1+1+1=7(字节)(魔数+版本号+序列化算法+指令)；
2.然后需要关注的是，长度域的长度是多少，这里显然是4字节。有了长度偏移量和长度域的长度，就可以构造一个拆包器。new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 7, 4);其中，第一个参数指的是数据包的最大长度，第二个参数指的是长度域的偏移量，第三个参数指的是长度域的长度，这样一个拆包器写好之后，只需要在pipeline的最前面加上这个拆包器。
服务端
ch.pipeline().addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 7, 4));
ch.pipeline().addLast(new PacketDecoder());
ch.pipeline().addLast(new LoginRequestHandler());
ch.pipeline().addLast(new MessageRequestHandler());
ch.pipeline().addLast(new PacketEncoder());
客户端
ch.pipeline().addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 7, 4));
ch.pipeline().addLast(new PacketDecoder());
ch.pipeline().addLast(new LoginResponseHandler());
ch.pipeline().addLast(new MessageResponseHandler());
ch.pipeline().addLast(new PacketEncoder());
这样，在后续PacketDecoder进行decode操作的时候，ByteBuf就是一个完整的自定义协议数据包。
拒绝非本协议连接：魔数




总结：
1.通过一个个例子来理解了为什么要有拆包器。说白了，拆包器的作用就是根据我们的自定义协议，把数据拼装成一个个符合我们自定义数据包大小的ByteBuf，然后送到我们自定义协议的解码器中去解码。
2.Netty 自带的拆包器包括 基于固定长度的拆包器、基于换行符的拆包器、自定义分隔符的拆包器另外还有一种最重要的基于长度域的拆包器。通常Netty自带的拆包器已完全满足我们的需求，无需重复造轮子。
3.基于Netty自带的拆包器，我们可以在拆包前判断当前连上来的客户端是否是支持自定义协议的客户端，如果不是，可尽早关闭，节省资源。(魔数)
************************
channelHandler的生命周期：
1.handlerAdded()：指的是当检测到新连接后，调用ch.pipeline().addLast(new LifeCyCleTestHandler());之后的回调，表示在当前的channel中，已经成功添加了一个handler处理器。
2.channelRegistered()：这个回调方法，表示当前的channel的所有的逻辑处理已经和某个NIO线程建立了绑定关系，(Netty里面是使用了线程池的方式，只需要从线程池里面去抓取一个线程，然后绑定在这个channel上即可，这里的NIO线程通常指的是NioEventLoop)
3.ChannelActive()：当channel的所有业务逻辑链准备完毕(也就是说channel的pipeline中已经添加完所有的handler)以及绑定好一个NIO线程之后，这条连接算是真正激活了，接下来就会回调到此方法。
4.channelRead()：另一端发来数据，每次都会调用此方法，表示有数据可读。
5.channelReadComplete()：每次读完一次完整的数据之后，就会回调该方法，表示数据读取完毕。
当channel被关闭时，ChannelHandler回调方法的执行顺序：(其实就是新连接建立的时候的逆操作)
6.channelInactive()：表面上这条连接已经被关闭了，这条连接在TCP层面已经不再是ESTABLSH状态了。
7.channelUnregistered()：既然连接已经被关闭，那么与这条连接绑定的线程就不需要对这条连接负责了，这个回调就表示   与这条连接对应的NIO线程移除掉对这条连接的处理。
8.handlerRemoved()：移除掉这条连接上添加的所有的业务逻辑处理器。

ChannelInitializer的实现原理：
我们在给新连接定义handler的时候，其实只是通过childHandler()方法给新连接设置了一个handler，这个handler就是ChannelInitiallizer，而在ChannelInitializer的initChannel()方法里面，我们通过channel对应的pipeline，然后往里面塞handler。这里的ChannelInitializer其实就利用了Netty的handler生命周期中的channelRegistered()与handlerAdded()两个特性。
1.ChannelInitializer定义了一个抽象方法initChannel(),这个抽象方法由我们自行实现，我们在服务端启动的流程里面的实现逻辑就是往pipeline里面塞handler链。
2.handlerAdded()和channelRegistered()方法，都会尝试去调用initChannel()方法，initChannel()使用普通IfAbsent()来防止intChannel()被调用多次。
3.如果debugChannelInitializer的上述两个方法，就会发现，在handlerAdded()方法被调用的时候，channel其实已经和某个线程绑定上了，所以，就我们的应用程序来说，这里的channelRegistered()是多余的，那为什么这里还要尝试调用一次呢?可能是担心我们自己写的类继承自ChannelInitializer，然后覆盖掉handlerAdded()方法吧，这样的话，如果覆盖掉了。我们还有机会在channelRegistered()方法里面调用InitChannel()，然后把自定义的handler添加到pipeline中去。

生命周期中各方法举例(没有channelRegistered和channelUNRegistered)：
1.handlerAdded()和handlerRemoved()：这两个方法通常可以用在一些资源的申请和释放。
2.channelActive()和channelInActive()：对于应用程序来说，这两个方法表名的含义是TCP连接的建立和释放，通常我们在这两个回调里面统计单机的连接数，当channelActive()被调用，连接数加一，channelInActive()被调用，连接数减一。另外我们也可以在channelActive()里面实现对客户端连接ip的黑白名单的过滤。
3.channelRead()：
之前讲的拆包粘包原理，服务端根据自定义协议来进行拆包，就是在这个方法里，每次读到一定的数据，都会累加到一个容器里面，然后判断能否拆出来一个完整的数据包，如果够的话就拆了之后继续往下传递。
4.channelReadComplete()：之前我们向另一端写数据的时候，都是用writeAndFlush()方法写并刷新到底层，其实这种方法并不是特别搞笑，我们可以在调用writeAndFlush()方法的地方调用write()方法，然后在这个方法里面统一刷新，调用ctx.channel.flush()方法，如果对性能要求没有那么高，那么不用管这些，直接writeAndFlush也可以。
总结：
1.剖析了ChannelHandler(主要是ChannelInBoundHandler)的各个回调方法，连接的建立和关闭，执行回调方法有个逆向的过程。
2.每一种回调方法都有他各自的用法，但有的时候某些回调方法的使用边界有些模糊，这就需要我们根据需求其当的使用回调方法来处理不同的逻辑。
************************
实战：使用 channelHandler 的热插拔实现客户端身份校验
(忽略了很多)
对于Netty的设计来说，handler其实可以看做是一段功能相对聚合的逻辑，然后通过pipeline把这些一个个小的逻辑聚合起来，串起一个功能完整的逻辑链。

总结：
1.如果有很多业务逻辑的handler都要进行某些相同的操作，我们完全可以抽取出一个handler来单独处理
2.如果某个独立的逻辑在执行几次之后不需要执行了。那么我们可以通过ChannelHandler的热插拔机制来实现动态删除逻辑，应用程序性能处理更为高效
************************
实战：客户端互聊原理与实现
3. 一对一单聊实现
3.1 用户登录状态与 channel 的绑定
1.SessionUtil里面维持了一个userId->channel的映射map,调用bindSession()方法的时候，在map里面保存这个映射关系，SessionUtil还提供了getChannel()方法，这样就可以通过userId拿到对应的channel。
2.除了在map中维持映射关系以外，在bindSession()方法中，我们还给channel附上了一个属性，这个属性就是当前用户的Session，同时提供了getSession()方法，可以方便的拿到对应channel的会话信息。
3.这里的SessionUtil是将前面的LoginUtil重构了，其中的hasLogin()方法，只需要判断当前是否有用户的会话信息即可。
4.在LoginRequestHandler中，我们还重写了channelInactive()方法，用户下线之后，需要在内存里面自动删除userId和channel的映射关系，这是通过调用自定义的SessionUtil.unBindSession()来实现的。
3.2 服务端接收消息并转发的实现
1.服务端在接收到客户端发来的消息之后，首先拿到当前用户，也就是消息发送方的会话信息
2.在拿到消息发送方的会话信息之后，构造一个发送给客户端的消息对象MessageResponsePacket，填上消息发送方的用户标识，昵称，消息内容。
3.通过消息接收方的标识拿到对应的channel
4.如果消息接收方当前是登录状态，直接发送，如果不在线，控制台打印出一条警告消息
这里，服务端的功能相当于转发：收到一条消息之后，构建一条发送给另一个客户端的消息，接着拿到另一个客户端的channel，然后通过writeAndFlush()写出。
3.3 客户端收消息的逻辑处理
客户端接收到消息之后，只是把当前的消息打印出来(这里把发送方的用户标识打印出来是为了方便我们在控制台回消息的时候，可以直接复制，到了这里，所有的核心逻辑其实已经完成了)，还差最后一环：在客户端的控制台进行登录和发送消息逻辑。
3.4 客户端控制台登录和发送消息
1.在客户端启动的时候，起一个线程，如果当前用户还未登陆，在控制台输入一个用户名，然后构造一个登陆数据包发送给服务器，发完之后，设置一个等待超时时间，可以当做是登录逻辑的最大处理时间，这里就简单粗暴一点了。
2.如果当前用户是登录状态。可以在控制台输入消息接收方的userId，然后空格，再输入消息的具体内容，然后，我们就可以构建一个消息数据包，发送到服务端。
总结：
1.定义一个会话类Session用于维持用户的登录信息，用户登录的时候绑定Session与channel，用户登出或者断线的时候解绑Session和Channel。
2.服务端处理消息的时候，通过消息接收方的标识，拿到消息接收方的channel，调用writeAndFlush()将消息发送给消息接收方。
************************
实战：群聊的发起与通知
2.控制台程序重构
2.1 创建控制台命令执行器
首先，把在控制台要执行的操作抽象出来，抽象出一个接口
ConsoleCommand.java

public interface ConsoleCommand {
    void exec(Scanner scanner, Channel channel);
}
2.2 管理控制台命令执行器(不粘代码了)
1.在这个管理类中，把所有控制台要管理的指令塞到map中
2.执行具体操作的时候哦，现获取控制台输入的第一个指令，这里以字符串代替比较清晰，然后通过这个指令拿到对应的控制台命令执行器执行。

总结：
1.群聊的原理和单聊类似，都是通过标识拿到channel
2.本小节，重构了控制台的程序结构，在实际带有UI的IM应用中，输入的第一个指令其实就是对应点击UI的某些按钮或者菜单的操作
3.通过ChannelGroup，可以很方便的对一组channel进行批量操作。
************************
实战：群聊的成员管理(加入与退出、获取成员列表)

总结：添加一个服务端与客户端交互的新功能需要遵循以下步骤：
1.创建控制台指令对应的ConsoleCommand并添加到ConsoleCommandmanager
2.控制台输入指令和数据之后填入协议对应的指令数据包-xxxRequestPacket，将请求写到服务端
3.服务端对应的创建xxxRequestPacketHandler并添加到服务端的pipeline中，在xxxRequestPacketHandler处理完之后，构造对应的xxxResponsePacket发送给客户端。
4.客户端创建对应的xxxResponsePacketHandler，并添加到客户端的pipeline中，最后在xxxResponsePacketHandler中完成响应的处理。
5.最后！最容易忽略的一点就是，新添加的xxxPacket，别忘了完善编解码器PacketCodeC中的packetTypeMap！
************************
实战：群聊消息的收发以及Netty性能优化

3.共享handler
在使用Netty完成了IM的核心功能之后，我们发现服务端的pipeline链里面已经有了12 个handler,其中，与指令相关的handler有9个；Netty在这里的逻辑是：每有新的连接到来的时候，都会调用ChannelInitializer的initChannel()方法，然后这里的9个指令相关的handler都会被new一次；而且，这里面的每个指令handler的内部都没有成员变量，也就是无状态的，完全可以使用单例模式，即调用pipeline().addLast()的时候，都直接使用单例，无需每次都new，提高效率，减少很多小对象的创建。

4.
ps：是不是明确知道后续无序OutBound处理(直接从编码出发)的话，就可可以直接使用ctx.writeAndFlush()，反之则要使用ctx.channel().writeAndFlush()?    也可以这么理解的。
5.

6.

7.如何准确统计处理时长
通常，应用程序都有统计某个操作响应时间的需求，比如，基于这个例子
protected void channelRead0(ChannelHandlerContext ctx, T packet) {
    threadPool.submit(new Runnable() {
        long begin = System.currentTimeMillis();
        // 1. balabala 一些逻辑
        // 2. 数据库或者网络等一些耗时的操作
        // 3. writeAndFlush()
        // 4. balabala 其他的逻辑
        long time =  System.currentTimeMillis() - begin;
    })
}
这种做法是不推荐的，因为writeAndFlush()这个方法如果在非NIO线程(这里，其实是在业务线程中调用了该方法)中执行，它是一个异步操作，调用之后，其实是会立即返回的，但是它并不代表操作已经执行完毕了，剩下的所有操作，都是通过Netty内部有一个任务队列异步执行的，因此，这里的writeAndFlush()执行完毕之后，并不代表相关的逻辑：比如事件传播、编码等逻辑执行完毕，值是代表Netty接受了这个任务，那么如何判断writeAndFlush()执行完毕呢？可以通过添加监听器的方式，以返回的future来判断
protected void channelRead0(ChannelHandlerContext ctx, T packet) {
    threadPool.submit(new Runnable() {
        long begin = System.currentTimeMillis();
        // 1. balabala 一些逻辑
        // 2. 数据库或者网络等一些耗时的操作
        
        // 3. writeAndFlush
        xxx.writeAndFlush().addListener(future -> {
            if (future.isDone()) {
                // 4. balabala 其他的逻辑
                long time =  System.currentTimeMillis() - begin;
            }
        });
    })
}
writeAndFlush()方法会返回一个ChannelFuture对象，给这个对象添加一个监听器，然后在回调方法里面，监听这个方法的执行结果，进而在执行其他逻辑，最后统计耗时，这样统计出来的耗时才是最准确的。在Netty里面很多方法都是异步的操作，在业务线程中如果要统计这部分操作的时间，都需要使用监听器回调的方式来统计耗时。但是如果在NIO线程中调用，则不需要这么干。
8.总结
1.实现了群聊消息的最后一个部分：群聊消息的收发
2.所有的指令都实现之后，handler已经非常臃肿庞大了，然后，通过单例模式改造、编解码器合并、平行指令handler合并、慎重选择两种类型的writeAndFlush()的方法来压缩优化。
3.在handler处理中，如果有耗时的操作，我们需要把这些操作都丢到自定义的业务线程池中处理，因为NIO线程是会有很多channel共享的，我们不能阻塞他们。
4.对于统计耗时的场景，如果在自定义业务线程中调用类似writeAndFlush()的异步操作，需要通过添加监听器的方式来统计。

************************
实战：心跳与空闲检测
1.
2.服务端空闲检测：
对于服务端来说，客户端的连接如果出现假死，那么服务端将无法收到客户端的数据，也就是说如果能一直收到客户端发来的数据，那么可以说明这条连接还是活的，因此，服务端对于连接假死的应对策略就是空闲检测。
空闲检测：指的是每隔一段时间，检测这段时间内是否有数据读写，简化一下，我们的服务端只需要检测一段时间内，是否收到过客户端发过来的数据即可，Netty自带额IdleStateHandler就可以实现这个功能。
IMIdleStateHandler.java

public class IMIdleStateHandler extends IdleStateHandler {

    private static final int READER_IDLE_TIME = 15;

    public IMIdleStateHandler() {
        super(READER_IDLE_TIME, 0, 0, TimeUnit.SECONDS);
    }

    @Override
    protected void channelIdle(ChannelHandlerContext ctx, IdleStateEvent evt) {
        System.out.println(READER_IDLE_TIME + "秒内未读到数据，关闭连接");
        ctx.channel().close();
    }
}
-1.首先，这给类中的构造函数，它调用父类IdleStateHandler的构造函数，有四个参数，其中第一个表示读空闲时间，指的是如果在这段时间内没有读取到数据，就表示连接假死，第二个参数表示写空闲时间，指的是 如果在这段时间内没有写数据，就表示连接假死，第三个参数是 读写空闲时间，表示在这段时间内如果没有产生数据读或者写，则表示连接假死。写空闲和读写空闲为0，表示我们不关心这两类条件；最后一个参数表示时间的单位。在上面的例子中：如果15秒没有读到数据，就表示连接假死。
-2.连接假死之后会回调channelIdle()方法，我们的做法是在这里面打印消息，并手动关闭连接。
然后，我们把这个handler插入到服务端pipeline的最前面，为什么放在最前面？因为如果放在最后面的话，如果我们的连接读到了数据，但是在InBound的传播过程中出现了错误过着数据处理完毕就不往后传递了(我们的应用程序属于这类)，那么最终会导致IMIdleStateHandler就会读不到数据，最终导致误判。
服务端的空闲检测时间完毕之后，能否就根据 在一段时间内没有收到客户端的数据，就判断连接假死呢？并不能，因为如果在这一段时间内客户端确实没有发送数据过来，但是连接还是ok的，那么这个时候服务端也不能关闭这条连接，为了防止服务端误判，还需要在客户端做点什么，比如 定时发送心跳。


3.客户端定时发送心跳
服务端在一段时间内没有收到客户端的数据，那么这种现象的原因可以分为下面两种
1.连接假死
2.非假死状态下确实没有发送数据
我们只要排除了第二种情况，就可以判断连接是假死的。那么要排查第二种情况，可以在客户端定期发送数据到服务端，通常这个数据包称做 心跳数据包，我们先定义好心跳数据包的请求和响应包。然后定义handler，定期发送心跳给服务端。

4.服务端回复心跳与客户端的空闲检测
客户端与服务端一样，在pipeline的最前方插入空闲检测IMIdelStateHandler，然后为了确定服务端是否假死，服务端也需要定期发送心跳给客户端，而其实之前客户端已经定期发送心跳了。所以服务端这边只要在收到心跳之后回复客户端，给客户端发送一个心跳响应即可。如果在一段时间内客户端没有收到服务端发来的数据，也可以判定这条连接为假死状态。
因此，服务端的pipeline中需要加上一个HeartBeatRequestHandler，这个handler的处理其实是无需登录的，所以，将他放在AuthHandler前面。
客户端检测到假死连接之后，断开连接，然后可以有一定的策略去重连，重新登录等，这里不展开了，留给读者自行实现。
5.总结
1.首先讨论了连接假死的相关现象以及产生原因
2.要处理假死问题首先要实现客户端与服务端定期发送心跳，在这里其实服务端只需要对客户端的定时心跳包进行回复。
3.客户端与服务端如果都需要检测假死，那么可以直接在pipeline的最前方插入一个自定义的IdleStateHandler，在chaneelIdler()方法里面自定义连接假死之后的逻辑。
4.通常空闲检测时间要比心跳的时间的两倍要长一些(可以3：1)，这也是为了排除偶发的公网抖动，防止误判。
************************
小测总结：
1.Netty是什么？
Netty其实可以看做是对BIO和NIO的封装，并提供良好的IO读写相关的API，另外还提供了非常多的开箱即用的handler，工具类等。
2.服务端和客户端的启动
Netty提供了两大启动辅助类，ServerBootstrap和Bootstrap，他们的启动参数类似，都是分为
  1.配置线程模型，配置IO类型
  2.配置TCP参数，attr属性
  3.配置handler。server端除了配置handler，还需要配置childhandler，他是定义每条连接的处理器。
3.ByteBuf
Netty对二进制数据的抽象类ByteBuf，ByteBuf底层可以细分为堆内存和堆外内存，它的API要比jdk提供的ByteBuffer要更好用，ByteBuf所有的操作其实都是基于读指针和写指针来进行操作的，把申请到的一块内存划分为可读区、可写区，另外还提供了自动扩容的功能。
4.自定义协议拆包与编解码









************************
Selsector：
selector是一个可以监控多个channel的NIO组件，它负责决定哪些channel可以进行读和写，这样一个线程就可以管理多个channel，从而管理多个网络。因为线程的切换时比较消耗资源的，而且线程本身也会占用一些资源，所以线程越少越好。通过调用Selector.open()方法来创建一个Selector；为了使用带有channel的selector，必须要将channel注册进selector，注册操作可以通过调用SelectableChannel.register()方法来实现，如下所示：
channel.configureBlocking(false);
SelectionKey key = channel.register(selector, SelectionKey.OP_READ)
Channel必须是非阻塞模式，才能和selector一起使用，第二个参数代表你希望发生在channel 上的事件
(未完待续)


























-pipeline顺序？
评论中的类似参考答案
评论：
Q：IMHandler添加的规则是什么?不太理解，平行指令处理handler是什么意思？为什么有的hanlder加进来了，有的没有
闪电侠回答：
A：平行指令处理handler指的是，在这组平行指令处理器中，每次仅有一个 handler 会处理 指令packet，也就是说只要有一个 handler 能处理指令 packet，其他的就不能再处理了，然后这边登录和鉴权比较特殊（如果没有登录，每次数据包过来都要鉴权，并且鉴权，无法加到平行处理器中，因为与后续的handler有个先后关系），所以没有加到 IMhandler里



//将字符串切割成字符数组，然后利用Arrays.asList()方法，转换成List集合
String a = "asd-qwe-zxc-dgf";
Arrays.asList(a.split("-"))




6位标志位

URG: 标识紧急指针是否有效 
ACK: 标识确认序号是否有效 
PSH: 用来提示接收端应用程序立刻将数据从tcp缓冲区读走 
RST: 要求重新建立连接. 我们把含有RST标识的报文称为复位报文段 
SYN: 请求建立连接. 我们把含有SYN标识的报文称为同步报文段 
FIN: 通知对端, 本端即将关闭. 我们把含有FIN标识的报文称为结束报文段





TCP/IP协议族包括运输层、网络层、链路层。socket是一个接口，在用户进程与TCP/IP协议中间充当中间人，完成协议的书写，用户只需要理解应用socket接口。





















************************
************************



************************
Ps：可以先不整理笔记，学完实战之后，统一整理，顺便复习.









************************
************************
Java netty的option(ChannelOption.SO_BACKLOG, backLog)什么意思？

这个都是socket的标准参数，并不是netty自己的。

具体为：
1.ChannelOption.SO_BACKLOG, 1024
BACKLOG用于构造服务端套接字ServerSocket对象，标识当服务器请求处理线程全满时，用于临时存放已完成三次握手的请求的队列的最大长度。如果未设置或所设置的值小于1，Java将使用默认值50。

2.ChannelOption.SO_KEEPALIVE, true
是否启用心跳保活机制。在双方TCP套接字建立连接后（即都进入ESTABLISHED状态）并且在两个小时左右上层没有任何数据传输的情况下，这套机制才会被激活。

3.ChannelOption.TCP_NODELAY, true
在TCP/IP协议中，无论发送多少数据，总是要在数据前面加上协议头，同时，对方接收到数据，也需要发送ACK表示确认。为了尽可能的利用网络带宽，TCP总是希望尽可能的发送足够大的数据。这里就涉及到一个名为Nagle的算法，该算法的目的就是为了尽可能发送大块数据，避免网络中充斥着许多小数据块。
 TCP_NODELAY就是用于启用或关于Nagle算法。如果要求高实时性，有数据发送时就马上发送，就将该选项设置为true关闭Nagle算法；如果要减少发送次数减少网络交互，就设置为false等累积一定大小后再发送。默认为false。

4.ChannelOption.SO_REUSEADDR, true
SO_REUSEADDR允许启动一个监听服务器并捆绑其众所周知端口，即使以前建立的将此端口用做他们的本地端口的连接仍存在。这通常是重启监听服务器时出现，若不设置此选项，则bind时将出错。
SO_REUSEADDR允许在同一端口上启动同一服务器的多个实例，只要每个实例捆绑一个不同的本地IP地址即可。对于TCP，我们根本不可能启动捆绑相同IP地址和相同端口号的多个服务器。
SO_REUSEADDR允许单个进程捆绑同一端口到多个套接口上，只要每个捆绑指定不同的本地IP地址即可。这一般不用于TCP服务器。
SO_REUSEADDR允许完全重复的捆绑：当一个IP地址和端口绑定到某个套接口上时，还允许此IP地址和端口捆绑到另一个套接口上。一般来说，这个特性仅在支持多播的系统上才有，而且只对UDP套接口而言（TCP不支持多播）

5.ChannelOption.SO_RCVBUF  AND  ChannelOption.SO_SNDBUF 
定义接收或者传输的系统缓冲区buf的大小，

6.ChannelOption.ALLOCATOR 
Netty4使用对象池，重用缓冲区
bootstrap.option(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT);
bootstrap.childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT);



















************************
************************
--------
--------


















　　
--------
--------
--------

　　偏偏秉烛夜游
　　午夜星辰 似奔走之友
　　爱你每个结痂伤口
　　酿成的陈年烈酒
　　入喉尚算可口
　　怎么泪水还偶尔失守
　　邀你细看心中缺口
　　裂缝中留存 温柔

　　此时已莺飞草长(zhǎng)爱的人正在路上
　　我知他风雨兼程途经日暮不赏
　　穿越人海只为与你相拥
　　此刻已皓月当空爱的人手捧星光
　　我知他乘风破浪去了黑暗一趟
　　感同身受给你救赎热望

　　知道你不能 还要你感受
　　让星光加了一点彩虹
　　让樱花偷偷 吻你额头
　　让世间美好与你环环相扣

　　此时已莺飞草长(zhǎng)爱的人正在路上
　　我知他风雨兼程途经日暮不赏
　　穿越人海只为与你相拥
　　此刻已皓月当空爱的人手捧星光
　　我知他乘风破浪去了黑暗一趟
　　感同身受给你救赎热望

　　此时已莺飞草长爱的人正在路上
　　我知他风雨兼程途经日暮不赏
　　穿越人海只为与你相拥
　　此刻已皓月当空爱的人手捧星光
　　我知他乘风破浪去了黑暗一趟
　　感同身受给你救赎热望

   　知道你不能 还要你感受
　　让星光加了一点彩虹
　　当樱花开的纷纷扬扬
　　当世间美好与你环环相扣
































